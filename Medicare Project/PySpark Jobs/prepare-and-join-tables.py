"""
This script reads data from 2 local file medicare-ffs-geographic-variation-2014-2022.csv as master file
and file generated by job ip-hospitals-local-load.py
Clean and filter data for the year 2022. Read lookup for State names from state_lookup.csv and replace state names in master file 
Perform join on above 2 files and display required information.
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import DecimalType

spark = SparkSession.builder.master("local[*]").appName("Read Local CSV File") \
        .getOrCreate()

df_aggregate = spark.read.csv("D:/Data/ip-hospitals-local-op-csv/*.csv",header=True, inferSchema=True)
df_aggregate.printSchema()

agg_count=df_aggregate.count()
print("Count of aggregate records: ",agg_count)

df_master = spark.read.csv("D:/Data/medicare-ffs-geographic-variation-2014-2022.csv",header=True, inferSchema=True)
df_select = df_master.select('YEAR','BENE_GEO_LVL','BENE_GEO_DESC','BENE_AGE_LVL','IP_MDCR_PYMT_AMT','BENES_TOTAL_CNT','BENES_FFS_CNT','TOT_MDCR_PYMT_AMT')
df_select.printSchema()

# Lookup data
df_lookup = spark.read.csv("D:/Data/state_lookup.csv",header=True, inferSchema=True)

# Filter invalid and duplicate data
df_state = df_select.filter(~col("BENE_GEO_LVL").rlike("County"))
df_all_age = df_state.filter(col("BENE_AGE_LVL").rlike("All"))

# Filter data to take only 2022 year's data
df_filtered = df_all_age.filter(col('YEAR').like('%2022'))

df_modified = df_filtered.filter(~col('BENE_GEO_DESC').rlike("Territory")) \
                          .filter(~col('BENE_GEO_DESC').rlike("ZZ")) 

df_lookup = broadcast(df_lookup)

# Replace bene_geo_desc with state name from lookup
df_modified = df_modified.alias("a").join(df_lookup.alias("b"),col("a.BENE_GEO_DESC").startswith(col("b.geo_desc")),"left")

df_modified = df_modified.withColumn(
                                    "BENE_GEO_DESC",
                                     when(col("provider_geo_desc").isNotNull(),col("provider_geo_desc")).otherwise(col("BENE_GEO_DESC"))
                                     )
df_modified = df_modified.drop("geo_desc","provider_geo_desc","BENE_GEO_LVL","BENE_AGE_LVL","YEAR")

count=df_modified.count()
print("Count of 2022 records: ",count)

# Perform inner join on df_modified and df_aggregate
df_joined = df_aggregate.join(df_modified, df_aggregate.Rndrng_Prvdr_Geo_Desc==df_modified.BENE_GEO_DESC, "inner")
            
df_output = df_joined.drop("BENE_GEO_DESC")           
out_count=df_output.count()
print("Count of joined records: ",out_count)

df_output.show() 

# Write data to csv file
df_output.coalesce(1).write.mode("overwrite").option("header", True).csv("D:/Data/joined-pymt-summary-csv")

spark.stop()